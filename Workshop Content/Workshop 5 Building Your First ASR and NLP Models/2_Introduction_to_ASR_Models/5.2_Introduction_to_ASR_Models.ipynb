{"cells":[{"cell_type":"markdown","metadata":{"id":"4feY2JN6GrP9"},"source":["# Building Your First Automatic Speech Recognition (ASR) model"]},{"cell_type":"markdown","metadata":{"id":"nIqi4tE3GrP-"},"source":["This notebook will introduce you to using a pre-trained Wave2Vec 2.0 model for Automatic Speech Recognition. We will explore the basics of ASR technology and take a hands-on approach to fine-tune the pre-trained Wav2Vec 2.0 model on a limited dataset. This session is designed to give you a practical understanding of the challenges and steps involved in developing an ASR model.\n","\n","Do note that our use of such a small dataset here is just meant to give you an indication of what the process would look like; scaling up the dataset size to a more sensible and useful one is an exercise left to the participant."]},{"cell_type":"markdown","metadata":{"id":"9yAvRDgyGrP-"},"source":["#### Installation Guide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8maZvld4GrP-"},"outputs":[],"source":["!pip install librosa jiwer torchaudio jsonlines datasets accelerate"]},{"cell_type":"code","source":["# on colab, download data\n","!gdown -O ./data --folder https://drive.google.com/drive/folders/1iTpHzVWh8TydoCkd75RGP8TTlMk8SMZJ"],"metadata":{"collapsed":true,"id":"l8R0teTHHFcg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7L9WnKHvGrP-"},"source":["<img src=\"https://lh3.googleusercontent.com/d/1-24xmHwUyx1OuM2qk-TcMLUutjVsMgkd\" alt=\"drawing\" width=\"650\">"]},{"cell_type":"markdown","metadata":{"id":"J9i99pxjGrP-"},"source":["### Using a pre-trained model for ASR"]},{"cell_type":"markdown","metadata":{"id":"mcyDjroOGrP-"},"source":["Wave2Vec 2.0 is a powerful model developed by Facebook for converting speech to text. Here, we'll demonstrate fine-tuning this modelwith minimal additional training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcR3LSaaGrP_"},"outputs":[],"source":["import jsonlines\n","import torchaudio\n","from datasets import Dataset\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n","from pathlib import Path\n","import torch\n","import librosa\n","import IPython.display as ipd\n","import jiwer"]},{"cell_type":"markdown","metadata":{"id":"rtK8ZVGbGrP_"},"source":["### Using Wav2Vec2 to transcribe an audio file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pb1RgIZcGrP_"},"outputs":[],"source":["model_name = \"facebook/wav2vec2-base-960h\"\n","processor = Wav2Vec2Processor.from_pretrained(model_name)\n","model = Wav2Vec2ForCTC.from_pretrained(model_name)\n","\n","audio_file = 'data/audio_1.wav'\n","audio_input, sample_rate = librosa.load(audio_file, sr=16000)\n","\n","input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n","\n","with torch.no_grad():\n","    logits = model(input_values).logits\n","\n","predicted_ids = torch.argmax(logits, dim=-1)\n","transcription = processor.batch_decode(predicted_ids)[0]\n","\n","print(\"Transcription:\", transcription)"]},{"cell_type":"markdown","metadata":{"id":"LEAlKKyfGrP_"},"source":["**Wav2Vec2 Processor:**\n","\n","The Wav2Vec2 processor is responsible for converting raw audio signals into input features that the Wav2Vec2 model can understand.\n","It consists of a feature extractor and a tokenizer.\n","The feature extractor processes the raw audio waveform, typically by performing preprocessing steps such as resampling and normalization.\n","The tokenizer tokenizes the processed audio into input tokens suitable for the model.\n","In our code, we use the Wav2Vec2Processor to instantiate the processor from the pretrained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfInBIubGrP_"},"outputs":[],"source":["# Play the loaded audio file\n","audio_data, sampling_rate = librosa.load(audio_file, sr=None)\n","waveform, sample_rate = torchaudio.load(audio_file)\n","ipd.Audio(waveform, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{"id":"X5h26hlVGrP_"},"source":["### Evaluation of ASR Systems using Word Error Rate"]},{"cell_type":"markdown","metadata":{"id":"kz8g9FFdGrP_"},"source":["Word Error Rate (WER) is a crucial metric used to evaluate the performance of an Automatic Speech Recognition (ASR) system. It measures how accurately the ASR system transcribes spoken language into text by comparing the machine-generated transcription to a human-generated reference transcription.\n","\n","The formula for WER is:\n","\n","$$ WER = \\frac{S + D + I}{N} $$\n","\n","Where:\n","- \\( S \\) represents the number of substitutions, which occur when a word from the reference is replaced by a different word in the hypothesis.\n","- \\( D \\) represents the number of deletions, where a word from the reference is missing in the hypothesis.\n","- \\( I \\) represents the number of insertions, where a word not present in the reference appears in the hypothesis.\n","- \\( N \\) is the total number of words in the reference transcription.\n","\n","The WER gives us a percentage that reflects the proportion of errors (substitutions, deletions, and insertions) in the hypothesis compared to the total number of words in the reference. A WER of 0% means perfect transcription, while a higher WER indicates more discrepancies between the hypothesis and the reference."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMrgp1NOGrP_"},"outputs":[],"source":["# Example data\n","references = [\n","    \"this is a test\",\n","    \"HEADING IS TWO SIX ZERO, TARGET IS BLACK, WHITE, AND YELLOW COMMERCIAL AIRCRAFT, TOOL TO DEPLOY IS SURFACE-TO-AIR MISSILES\"\n","]\n","\n","hypotheses = [\n","    \"this is test\",\n","    \"HEADING HIS TWO STICK FERO TARGATIVE BLACK WHITE AND YELLOW COMMERCIAL AIR CRAFT TOOLED TO DEPOY IN CIRCUS AIR MISSILF\"\n","]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXJz5bk3GrP_"},"outputs":[],"source":["# Function to calculate WER\n","def calculate_wer(references, hypotheses):\n","    wer_scores = []\n","    for ref, hyp in zip(references, hypotheses):\n","        wer_score = jiwer.wer(ref, hyp)\n","        wer_scores.append(wer_score)\n","    return wer_scores\n","\n","# Calculate WER for each pair\n","wer_scores = calculate_wer(references, hypotheses)\n","\n","# Display the results\n","for i, score in enumerate(wer_scores):\n","    print(f\"Reference {i+1}: {references[i]}\")\n","    print(f\"Hypothesis {i+1}: {hypotheses[i]}\")\n","    print(f\"WER: {score:.2%}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"Cov9pu8kGrP_"},"source":["## Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"YMvVg0RiGrP_"},"source":["#### Setup and Loading Data\n","We start by setting up the environment and loading our training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"risV6jwnGrQA"},"outputs":[],"source":["# Define the path to the directory\n","data_dir = Path(\"data\")\n","\n","# Read data from a jsonl file and reformat it\n","data = {'key': [], 'audio': [], 'transcript': []}\n","with jsonlines.open(data_dir / \"asr.jsonl\") as reader:\n","    for obj in reader:\n","        if len(data['key']) < 10:  # Only keep the first 10 entries\n","            for key, value in obj.items():\n","                data[key].append(value)\n","\n","# Convert to a Hugging Face dataset\n","dataset = Dataset.from_dict(data)\n","\n","# Shuffle the dataset\n","dataset = dataset.shuffle(seed=42)\n","\n","# Split the dataset into training, validation, and test sets\n","train_size = int(0.8 * len(dataset))\n","val_size = int(0.1 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","train_dataset = dataset.select(range(train_size))\n","val_dataset = dataset.select(range(train_size, train_size + val_size))\n","test_dataset = dataset.select(range(train_size + val_size, train_size + val_size + test_size))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azDkZkJUGrQA"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rr6HZvxWGrQA"},"outputs":[],"source":["processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"]},{"cell_type":"markdown","metadata":{"id":"gVLSdAWSGrQA"},"source":["#### Preprocessing Audio and Label Data\n","Below, we define a preprocessing function preprocess_data that takes an input dictionary of examples (containing audio paths and transcripts) and preprocesses them for training. It loads audio files, processes them using the Wav2Vec2 processor, creates attention masks, and pads the labels to match the input length."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7VLzMpOGrQA"},"outputs":[],"source":["# Initially freeze all layers except the classifier layer\n","for param in model.parameters():\n","    param.requires_grad = False\n","for param in model.lm_head.parameters():\n","    param.requires_grad = True\n","\n","# Function to load and preprocess audio\n","def preprocess_data(examples):\n","    input_values = []\n","    attention_masks = []\n","    labels = []\n","\n","    for audio_path, transcript in zip(examples['audio'], examples['transcript']):\n","        speech_array, sampling_rate = torchaudio.load(data_dir / audio_path)\n","        processed = processor(speech_array.squeeze(0), sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n","\n","        # Process labels with the same processor settings\n","        with processor.as_target_processor():\n","            label = processor(transcript, return_tensors=\"pt\", padding=True)\n","\n","        input_values.append(processed.input_values.squeeze(0))\n","        # Create attention masks based on the input values\n","        attention_mask = torch.ones_like(processed.input_values)\n","        attention_mask[processed.input_values == processor.tokenizer.pad_token_id] = 0  # Set padding tokens to 0\n","        attention_masks.append(attention_mask.squeeze(0))\n","\n","        # Ensure labels are padded to the same length as inputs if needed\n","        padded_label = torch.full(processed.input_values.shape[1:], -100, dtype=torch.long)\n","        actual_length = label.input_ids.shape[1]\n","        padded_label[:actual_length] = label.input_ids.squeeze(0)\n","        labels.append(padded_label)\n","\n","    # Concatenate all batches\n","    examples['input_values'] = torch.stack(input_values)\n","    examples['attention_mask'] = torch.stack(attention_masks)\n","    examples['labels'] = torch.stack(labels)\n","\n","    return examples\n"]},{"cell_type":"markdown","metadata":{"id":"5Z7g-0uhGrQA"},"source":["**Padding:**\n","\n","- Since the Wav2Vec2 model expects inputs of fixed length, we need to pad the input tensors to ensure consistency. We pad both the input features and the labels (transcripts) to the same length to maintain alignment.\n","- Padding is done using PyTorch's `torch.full` function, which fills a tensor with a specified value to a specified shape. We calculate the actual length of the label sequence and pad it accordingly to match the length of the input features.\n","- Padding ensures that all input tensors within a batch have the same shape, allowing them to be efficiently processed in parallel.\n","\n","**Attention Masks:**\n","\n","- Attention masks are used to indicate which tokens in the input should be attended to by the model and which should be ignored. In the case of Wav2Vec2, we use attention masks to mask padding tokens.\n","- We create attention masks of the same shape as the input tensors, initialized with ones. We then set the elements corresponding to padding tokens to zero to mask them.\n","- This ensures that the model does not attend to the padding tokens during training or inference, improving efficiency. Attention masks are essential for maintaining the correct input-output alignment, especially when using batch processing."]},{"cell_type":"markdown","metadata":{"id":"Trv2OVdSGrQA"},"source":["#### Training Configuration\n","Define the training arguments for the Trainer, including the output directory, evaluation strategy, learning rate, batch size, number of epochs, and other training settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YQ0uwPwGrQA"},"outputs":[],"source":["# Apply preprocessing\n","train_dataset = train_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=train_dataset.column_names)\n","val_dataset = val_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=val_dataset.column_names)\n","test_dataset = test_dataset.map(preprocess_data, batched=True, batch_size=1, remove_columns=test_dataset.column_names)\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"steps\",\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=1,  # Reduce to one for simplicity\n","    num_train_epochs=10,\n","    weight_decay=0.005,\n","    save_steps=500,\n","    eval_steps=500,\n","    logging_steps=10,\n","    load_best_model_at_end=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"i565C0iCGrQA"},"source":["#### Training & Evaluation\n","Conduct 10 epochs of training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpKUlCYzGrQA"},"outputs":[],"source":["# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,  # Use the validation dataset for evaluation\n","    tokenizer=processor.feature_extractor\n",")\n","\n","# Train the model\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"hXj7OWMlGrQA"},"source":["This script provides a very basic example of fine-tuning a Wav2Vec2 model on a few data points.\n","When fine-tuning a complex model like Wav2Vec 2.0 on an extremely limited dataset the model's performance is likely to be highly unpredictable and generally poor. However, we can still observe the validation loss decreasing over 10 epochs.\n","\n","### Conclusion\n","In this workshop we took our first steps into ASR. Starting with a pre-trained model, we learnt to fine-tune it on a small dataset. Given the complexity and the depth of models like Wav2Vec 2.0, they require substantial data to adapt their pre-trained knowledge to new tasks or domains effectively. In a real-world scenario, one would need to manage larger datasets and more sophisticated training routines involving, more epochs, consider freezing some layers, hyper-parameter tuning, validation and regularization based on performance metrics.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[{"file_id":"1oo2Med5tLAWIOVY_6vRbHFhGugEYhqK4","timestamp":1715488878889}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}