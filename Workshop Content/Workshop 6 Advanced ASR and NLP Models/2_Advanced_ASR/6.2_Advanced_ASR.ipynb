{"cells":[{"cell_type":"markdown","metadata":{"id":"Xuswe-YFHdnX"},"source":["# Advanced ASR Techniques\n","\n","In this notebook, we'll cover the essential steps of handling audio data for speech recognition tasks, dive into MFCCs transcribing speech using Whisper and noise reduction"]},{"cell_type":"markdown","metadata":{"id":"2W4XMLiTHdnY"},"source":["#### Installation Guide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnrRJ2NDHdnY"},"outputs":[],"source":["!pip install librosa jiwer openai-whisper torch torchaudio matplotlib noisereduce"]},{"cell_type":"code","source":["# on colab, download data\n","!gdown -O ./data --folder https://drive.google.com/drive/folders/1fDQhiDjTWiw80pZ7oFErnAMWEjIQN99p"],"metadata":{"id":"0gCgn9VzOQZy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSylXQMBHdnZ"},"source":["### Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhLi1lSAHdnZ"},"outputs":[],"source":["import librosa\n","import IPython.display as ipd\n","\n","# Load an audio file as a floating point time series\n","audio_path = 'data/audio.wav'\n","audio_data, sampling_rate = librosa.load(audio_path, sr=None)  # sr=None ensures original sampling rate is preserved\n","\n","# Display basic information about the audio\n","print(f\"Audio duration: {audio_data.shape[0] / sampling_rate:.2f} seconds\")\n","print(f\"Sampling rate: {sampling_rate} Hz\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RD4PtzmRHdnZ"},"outputs":[],"source":["import torch\n","import torchaudio\n","import matplotlib.pyplot as plt\n","\n","def load_audio(file_path):\n","    waveform, sample_rate = torchaudio.load(file_path)\n","    print(f\"Loaded audio file '{file_path}'\")\n","    print(f\"Sample rate: {sample_rate}\")\n","    print(f\"Waveform tensor shape: {waveform.shape}\")\n","    return waveform, sample_rate\n","\n","# Example usage\n","audio_path = file_path = 'data/audio.wav'\n","waveform, sample_rate = load_audio(audio_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rpM9gFMJHdnZ"},"outputs":[],"source":["# Play the loaded audio file\n","ipd.Audio(waveform, rate=sampling_rate)\n"]},{"cell_type":"markdown","metadata":{"id":"dizMx7D-Hdna"},"source":["Sampling is the process of recording the amplitude of a sound wave at discrete intervals. The sample rate defines how many samples per second are recorded.\n","The choice of sample rate affects the accuracy of the speech features captured. A higher sample rate can capture more detail but requires more computational resources.\n"]},{"cell_type":"markdown","metadata":{"id":"nWfAqWuOHdna"},"source":["### Visualizing Audio Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pREyKpMpHdna"},"outputs":[],"source":["# Plotting the waveform\n","plt.figure(figsize=(12, 4))\n","plt.plot(audio_data)\n","plt.title('Audio Waveform')\n","plt.xlabel('Time (samples)')\n","plt.ylabel('Amplitude')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"GMO4yZrPHdna"},"source":["### Audio Pre-processing\n","\n","Speech recognition systems often require specific audio preprocessing steps to improve the accuracy of recognition. Common steps include:\n","- Resampling: Changing the sample rate of the audio file. This is often necessary to standardize the input data to a consistent format expected by downstream processes or models.\n","- Normalization: Scaling the audio waveform so that the amplitude ranges within a specific interval, typically between -1 and 1. This helps in reducing disparities in volume and amplitude between different audio recordings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaMMypJ4Hdna"},"outputs":[],"source":["def preprocess_audio(waveform, new_sample_rate=16000):\n","    # Resample\n","    if waveform.shape[1] != new_sample_rate:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=new_sample_rate)\n","        waveform = resampler(waveform)\n","        print(f\"Resampled waveform to {new_sample_rate} Hz\")\n","\n","    # Normalize audio to [-1, 1]\n","    waveform = waveform / torch.max(torch.abs(waveform))\n","    return waveform\n","\n","waveform = preprocess_audio(waveform, new_sample_rate=16000)\n"]},{"cell_type":"markdown","metadata":{"id":"JsRI7YcTHdna"},"source":["Pre-processing standardizes the audio inputs, which helps in reducing variability that is not related to the linguistic content of the speech. For example, models trained with data at a consistent sample rate and amplitude are more stable and perform better in recognizing speech from various sources."]},{"cell_type":"markdown","metadata":{"id":"QVveCv1WHdna"},"source":["#### Feature Extraction\n","\n","Feature extraction is a crucial step in speech recognition. It transforms raw audio data into a more compact and representative format that is easier for ASR models to process. There are several features commonly used:\n","\n","- **Spectrograms**: A spectrogram is a visual representation of the spectrum of frequencies in a sound as they vary with time. It is obtained by applying a Fourier transform (FFT) across short time windows of the waveform.\n","\n","- **Mel Spectrograms**: This involves converting the frequency scale of the spectrogram to the Mel scale, which more closely approximates human auditory perception's response to different frequencies. The frequencies are spaced evenly according to the Mel scale, which is a perceptual scale of pitches judged by listeners to be equal in distance from one another.\n","\n","- Mel-frequency cepstral coefficients (**MFCCs**): MFCCs are perhaps the most widely used feature extraction method in speech recognition systems. They are derived from the logarithm of the Mel spectrogram followed by a discrete cosine transform (DCT). They provide a compact representation of the audio based on the power spectrum of a signal.\n","\n","**Role of `n_mfcc`**\n","\n","- `n_mfcc` Parameter: This parameter specifies how many of the resulting cepstral coefficients are returned after the DCT. The first coefficient, which represents the mean energy of the signal, is usually discarded because it carries less information about the spectral shape.\n","- Choosing `n_mfcc`: The number of coefficients to use is a trade-off between having enough coefficients to capture the important characteristics of the audio signal and not having so many that the dimensionality becomes too high, which could lead to overfitting in machine learning models. Typically, 12 to 20 MFCCs are used in speech recognition tasks.\n","- In the above code `n_mfcc` was set to 13. This means the MFCC computation process will extract the first 13 coefficients for each frame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvmrIq8nHdna"},"outputs":[],"source":["import torchaudio.transforms as T\n","\n","def extract_features(waveform, sample_rate, n_mfcc=13):\n","    # Compute standard Spectrogram\n","    spectrogram_transform = T.Spectrogram()\n","    spectrogram = spectrogram_transform(waveform)\n","\n","    # Compute Mel Spectrogram\n","    mel_spectrogram_transform = T.MelSpectrogram(sample_rate=sample_rate)\n","    mel_spectrogram = mel_spectrogram_transform(waveform)\n","\n","    # Compute MFCCs from Mel Spectrogram\n","    mfcc_transform = T.MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc, melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23})\n","    mfccs = mfcc_transform(waveform)\n","\n","    return spectrogram, mel_spectrogram, mfccs\n","\n","# Example usage (assuming 'waveform' and 'sample_rate' are already defined)\n","spectrogram, mel_spectrogram, mfccs = extract_features(waveform, sample_rate=16000)\n","\n","# Plot the standard Spectrogram\n","plt.figure(figsize=(10, 4))\n","plt.imshow(spectrogram.log2()[0,:,:].detach().numpy(), cmap='viridis', aspect='auto')\n","plt.title('Spectrogram')\n","plt.colorbar(format='%+2.0f dB')\n","plt.show()\n","\n","# Plot the Mel Spectrogram\n","plt.figure(figsize=(10, 4))\n","plt.imshow(mel_spectrogram.log2()[0,:,:].detach().numpy(), cmap='viridis', aspect='auto')\n","plt.title('Mel Spectrogram')\n","plt.colorbar(format='%+2.0f dB')\n","plt.show()\n","\n","# Plot the MFCCs\n","plt.figure(figsize=(10, 4))\n","plt.imshow(mfccs[0,:,:].detach().numpy(), cmap='viridis', aspect='auto')\n","plt.title('MFCC')\n","plt.colorbar()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"y5Rsz87MHdna"},"source":["**Importance for ASR:**\n","\n","These features are crucial because they transform the audio into a form that emphasizes characteristics important for distinguishing between different phonemes (sounds) in speech. This transformation is beneficial because:\n","\n","**Spectrograms** provide a time-frequency representation, capturing when certain sounds occur and their frequencies, which are essential cues for understanding speech.\n","\n","**Mel Spectrograms** emphasize perceptually important frequencies, making it easier for the model to focus on parts of the audio that are more likely to carry linguistic content.\n","\n","**MFCCs** capture the timbral and envelope characteristics of sound. They emphazie aspects of sound as perceived by the human ear. They are widely used in ASR because they effectively represent the vocal tract configuration that determines how we pronounce sounds, thus capturing the linguistic content effectively."]},{"cell_type":"markdown","metadata":{"id":"46jk6-UdHdnb"},"source":["### Whisper\n","\n","Whisper is an automatic speech recognition (ASR) system developed by OpenAI.  It's designed to provide robust transcription capabilities across various audio inputs, languages, and domains. Whisper utilizes transformer architecture to provide robust transcription capabilities across various languages and audio conditions. The model can handle different accents, background noises, and audio qualities, making it highly versatile for practical applications.\n","\n","<img src=\"https://lh3.googleusercontent.com/d/1JFd1l7FvXPGweUQZ1PmgqBYI_c_DpgGI\" alt=\"drawing\" width=\"750\">"]},{"cell_type":"markdown","metadata":{"id":"SgxWySVBHdnb"},"source":["Let us first load the Whisper model to transcribe an audio file to extract text directly from the original recording."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCDKG_VTHdnb"},"outputs":[],"source":["import whisper\n","model = whisper.load_model(\"base\")\n","result = model.transcribe('data/audio.wav', fp16 = False)\n","\n","result['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yslf4O1-Hdnb"},"outputs":[],"source":["# Play the loaded audio file\n","audio_path = 'data/audio.wav'\n","audio_data, sampling_rate = librosa.load(audio_path, sr=None)\n","\n","audio_path = file_path = 'data/audio.wav'\n","waveform, sample_rate = waveform, sample_rate = torchaudio.load(file_path)\n","ipd.Audio(waveform, rate=sampling_rate)\n"]},{"cell_type":"markdown","metadata":{"id":"owzA5_aSHdnb"},"source":["#### Noise Reduction\n"," To improve transcription quality, we reduce background noise using `noisereduce` libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUJIO69NHdnb"},"outputs":[],"source":["import noisereduce as nr\n","import librosa\n","import soundfile as sf\n","\n","# Load audio file\n","audio_path = 'data/audio.wav'\n","audio, rate = librosa.load(audio_path, sr=None)\n","\n","# Perform noise reduction\n","noisy_part = audio[0:int(rate*0.5)]  # Identify the noisy part\n","reduced_noise_audio = nr.reduce_noise(y=audio, sr=rate, y_noise=noisy_part)\n","\n","# Save the cleaned audio\n","clean_audio_path = 'data/cleaned_audio.wav'\n","sf.write(clean_audio_path, reduced_noise_audio, rate)\n","result = model.transcribe('data/cleaned_audio.wav', fp16 = False)\n","\n","result['text']"]},{"cell_type":"markdown","metadata":{"id":"bO5ymKEBHdnb"},"source":["- `audio[0:int(rate * 0.5)]`: This line isolates a portion of the audio data assumed to be noisy.\n","- The expression `int(rate * 0.5)` calculates the number of audio samples that represent the first 0.5 seconds of the recording. This initial segment is used as the reference noise profile.\n","- `nr.reduce_noise`: This function from the noisereduce library takes the original audio `(y=audio)` and the isolated noisy segment ` (y_noise=noisy_part)` to reduce background noise."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92WbfS9UHdnb"},"outputs":[],"source":["# Play the cleaned audio file\n","audio_path = 'data/cleaned_audio.wav'\n","waveform, sample_rate = load_audio(audio_path)\n","ipd.Audio(waveform, rate=sampling_rate)"]},{"cell_type":"markdown","metadata":{"id":"BkIYGvfqHdnb"},"source":["The difference between the original and cleaned transcriptions you've provided can be attributed to the effects of noise reduction and the improved clarity it brings to the speech recognition process."]},{"cell_type":"markdown","metadata":{"id":"0dP28iCqHdnb"},"source":["### Conclusion\n","\n","In this notebook, we learnt to extract relevant audio features such as Mel-Frequency Cepstral Coefficients (MFCCs) to better understand the nature of the audio. We saw how Whisper model's advanced transformer architecture, combined with appropriate audio pre-processing and noise reduction, can deliver accurate and reliable transcriptions even in noisy environments."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[{"file_id":"1D3E5UVWnbUKStyDBknaEf6yqhk15Ivuv","timestamp":1715489100902}]}},"nbformat":4,"nbformat_minor":0}