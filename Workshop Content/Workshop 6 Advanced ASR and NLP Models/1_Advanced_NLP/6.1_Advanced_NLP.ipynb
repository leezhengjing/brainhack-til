{"cells":[{"cell_type":"markdown","metadata":{"id":"XOb5LYGCHcmi"},"source":["# Advanced NLP Models for Question-Answering"]},{"cell_type":"markdown","metadata":{"id":"wc-1RiGqHcmj"},"source":["In this notebook, you'll explore some of the more advanced Transformer models for Question-Answering - BERT, T5, ALBERT, ELECTRA, and Longformer. This workshop provides an in-depth look into their architecture and applications. The subdomain of Question Answering in NLP is popular because of the wide range of applications in answering questions with a reference document. The solution to this problem is tackled by using a dataset that consists of an input text, a query, and the segment of text or span from the input text which contains the answer to the query. With the help of deep learning models, there has been a significant improvement in achieving human-level predictions from the data."]},{"cell_type":"markdown","metadata":{"id":"_lkZQkhzHcmj"},"source":["#### Installation Guide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xoa1iLGHcmj"},"outputs":[],"source":["!pip install transformers torch"]},{"cell_type":"markdown","metadata":{"id":"J-GOHBx4Hcmk"},"source":["### The Transformer Architecture\n","The transformer architecture was proposed in the paper Attention is All You Need. The encoders encode the input text, and the decoder processes the encodings to understand the contextual information behind the sequence. Each encoder and decoder in the stack uses an attention mechanism to process each input along with every other input for weighing their relevance, and generates the output sequence with the help of the decoder. The attention mechanism enables dynamic highlighting and understanding the features in the input text.\n","\n","<img src=\"https://lh3.googleusercontent.com/d/1kMk1ZgOi5FA2GpTODXCWjdXBfmfSr9A_\" alt=\"drawing\" height=\"450\">"]},{"cell_type":"markdown","metadata":{"id":"YrzRJjKgHcmk"},"source":["### BERT (Bidirectional Encoder Representations from Transformers)\n","\n","The huggingface `transformers` library provides nicely wrapped implementations for us to experiment with.\n","\n","\n","\n","Let's begin our experiments by first diving into a BERT model for understanding its performance. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained NLP model developed by Google that uses bidirectional attention to understand the context of words in a sentence. It can be fine-tuned for various tasks, including text classification, question answering, and language generation.\n","\n","<img src=\"https://lh3.googleusercontent.com/d/1ye4IvOzGM4b2ix1e2ByMntCUv1rM9G7n\" alt=\"drawing\" width=\"650\"><br><br>\n","<img src=\"https://lh3.googleusercontent.com/d/15D_t6bQ3ziz0b2pUIbGHxGTKlJSsbWe0\" alt=\"drawing\" width=\"450\">\n","<img src=\"https://lh3.googleusercontent.com/d/1e5UhuKT5v1Jn0nsqNmEeIA9SPQMGOgeH\" alt=\"drawing\" width=\"350\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmDndvRSHcmk"},"outputs":[],"source":["from transformers import BertTokenizer,AlbertTokenizer,AutoTokenizer, AutoModelForQuestionAnswering ,BertForQuestionAnswering, AlbertForQuestionAnswering\n","import torch\n","\n","model_name='bert-large-uncased-whole-word-masking-finetuned-squad'\n","model = BertForQuestionAnswering.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gf4TIfIHcmk"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(model_name)\n","question = \"Who discovered penicillin?\"\n","answer_text = \"Penicillin was discovered in 1928 by Scottish scientist Sir Alexander Fleming. His discovery led to the development of antibiotics, which have saved countless lives.\"\n","\n","input_ids = tokenizer.encode(question, answer_text)\n","tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","print(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JrF7gxCHcmk"},"outputs":[],"source":["def qa(question,answer_text,model,tokenizer):\n","  inputs = tokenizer.encode_plus(question, answer_text, add_special_tokens=True, return_tensors=\"pt\")\n","  input_ids = inputs[\"input_ids\"].tolist()[0]\n","\n","  text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","  print(text_tokens)\n","  outputs = model(**inputs)\n","  answer_start_scores=outputs.start_logits\n","  answer_end_scores=outputs.end_logits\n","\n","  answer_start = torch.argmax(\n","      answer_start_scores\n","  )  # Get the most likely beginning of answer with the argmax of the score\n","  answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n","  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n","\n","  # Combine the tokens in the answer and print it out.\"\"\n","  answer = answer.replace(\"#\",\"\")\n","\n","  print('Answer: \"' + answer + '\"')\n","  return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ag6N0Y9UHcmk"},"outputs":[],"source":["qa(question,answer_text,model,tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"A1AhOVDkHcmk"},"source":["### Advanced Transformer Models"]},{"cell_type":"markdown","metadata":{"id":"yE9296L_Hcmk"},"source":["#### T5\n","\n","T5, developed by Google Research, frames every NLP task as a text-to-text problem, providing a unified framework for tasks like translation, summarization, and question answering. It excels at these tasks by leveraging large-scale pretraining on diverse text data with a denoising objective."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WvPJJq6JHcmk"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","model_name = 't5-small'\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","def t5_qa(question, context, model, tokenizer):\n","    input_text = f\"question: {question} context: {context}\"\n","    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", add_special_tokens=True)\n","\n","    outputs = model.generate(inputs['input_ids'])\n","    answer = tokenizer.decode(outputs[0])\n","\n","    print('Answer:', answer)\n","\n","t5_qa(question, answer_text, model, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"0tAdC5g4Hcml"},"source":["#### ALBERT (A Lite BERT)\n","ALBERT, developed by Google Research, is a smaller, faster BERT variant with significantly reduced memory requirements. It achieves comparable performance to BERT by using parameter-sharing techniques and sentence-order prediction, making it ideal for scaling NLP applications."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGvRjCNIHcml"},"outputs":[],"source":["tokenizer=AlbertTokenizer.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n","model=AlbertForQuestionAnswering.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n","qa(question,answer_text,model,tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"uessiPRnHcml"},"source":["#### ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately):\n","ELECTRA, also by Google Research, replaces BERT's masked language modeling with a \"discriminator-generator\" approach. The generator corrupts input tokens, and the discriminator identifies altered tokens, training models efficiently to provide high-quality language representations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVjlus99Hcml"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n","\n","qa(question,answer_text,model,tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"s3RhjQN5Hcml"},"source":["### Long Document Question-Answering Using Standard Models\n"," The transformers model uses the help of the self-attention operation to provide meaningful results. As the length of the sequence increases, the computation scales drastically for this mechanism. If we use the standard BERT model as we used earlier, an error will be observed, indicating that the long sequence of inputs cannot be processed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jaa70cdwHcml"},"outputs":[],"source":["question=\"how many pilots did the ship have?\"\n","\n","answer_text=\"\"\"\n","Tug boats had spent several hours on Monday working to free the bow of the massive vessel after dislodging the stern earlier in the day.\n","Marine traffic websites showed images of the ship away from the banks of the Suez Canal for the first time in seven days following an around-the-clock international effort to reopen the global shipping lane.\n","There are still 422 ships waiting to go through the Suez Canal, Rabie said, adding that the canal's authorities decided the ships will be able to cross the canal on a first come first serve basis, though the ships carrying livestock were permitted to cross in the first convoy of the day.\n","The average number of ships that transited through the canal on a daily basis before the accident was between 80 to 90 ships, according to Lloyds List; however, the head of the Suez Canal Authority said that the channel will work over 24 hours a day to facilitate the passage of almost 400 ships carrying billions of dollars in freight.\n","The journey to cross the canal takes 10 to 12 hours and in the event the channel operates for 24 hours, two convoys per day will be able to successfully pass through.\n","Still, shipping giant Maersk issued an advisory telling customers it could take \"6 days or more\" for the line to clear. The company said that was an estimate and subject to change as more vessels reach the blockage or are diverted.\n","The rescue operation had intensified in both urgency and global attention with each day that passed, as ships from around the world, carrying vital fuel and cargo, were blocked from entering the canal during the crisis, raising alarm over the impact on global supply chains.\n","What its really like steering the worlds biggest ships\n","What it's really like steering the world's biggest ships\n","Promising signs first emerged earlier on Monday when the rear of the vessel was freed from one of the canal's banks.\n","People at the canal cheered as news of Monday's progress came in.\n","The Panama Maritime Authority said that according to preliminary reports Ever Given suffered mechanical problems that affected its maneuverability.\n","\n","The ship had two pilots on board during the transit.\n","\n","However, the owner of the vessel, Japanese shipping company Shoe Kisen insists that there had been no blackout resulting in loss of power prior to the ship’s grounding.\n","Instead, gusting winds of 30 knots and poor visibility due to a dust storm have been identified as the likely causes of the grounding, which left the boxship stuck sideways in a narrow point of the waterway.\n","\"\"\"\n","\n","from transformers import BertTokenizer,BertForQuestionAnswering\n","model_name='bert-large-uncased-whole-word-masking-finetuned-squad'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForQuestionAnswering.from_pretrained(model_name)\n","\n","qa(question,answer_text,model,tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"PX2fh6UXHcml"},"source":["We need another model which has been pre-trained to include a longer sequence of input documents, and also an architecture that can process the same.\n","\n","#### Longformer\n","Developed by the Allen Institute for AI, Longformer is a Transformer model designed to handle long documents. It uses a combination of local and global attention mechanisms to reduce memory requirements, allowing it to capture relationships in sequences of up to 4,096 tokens while maintaining efficient computational performance. Longformer is particularly suited for tasks involving lengthy documents like text classification, summarization, and question answering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUAnBoZGHcml"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n","qa(question,answer_text,model,tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"VsDxYQ6RHcmm"},"source":["### Conclusion\n","\n","In this notebook, we have learnt to apply various transformer-based models for question answering. T5's text-to-text approach, ALBERT's efficient parameter-sharing, ELECTRA's novel discriminator-generator learning, and Longformer's scalability are all vital contributions to modern NLP. As you continue experimenting and applying what you've learned, you'll find innovative ways to leverage these models in your projects."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[{"file_id":"1dQGaTi1g78wuU99lKKJCJt65WC8MoyY9","timestamp":1715489070903}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}